{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "import operator\n",
    "import itertools\n",
    "import pickle\n",
    "import tqdm\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import simpleclock\n",
    "import sklearn.metrics\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.abspath(''), \"data_cine_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = \"spacy\",\n",
    "                            tokenizer_language=\"fr_core_news_sm\",\n",
    "                            include_lengths=True)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.float, use_vocab=False, preprocessing=lambda x: float(x) / 5)\n",
    "# labels are linearly rescaled to a 0-1 range\n",
    "# todo: test if preprocessing data before isn't faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchtext.data.TabularDataset(path=data_path,\n",
    "                                        format=\"CSV\",\n",
    "                                        fields={\"critique\": (\"input\", TEXT), \"note\": (\"target\", LABEL)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = dataset.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = data_train.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"training data: {len(data_train)} examples.\n",
    "validation data: {len(data_valid)} examples.\n",
    "test data: {len(data_test)} examples.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.Vectors(\"cc.fr.300.vec\", os.path.join(os.path.expanduser(\"~\"), \"Downloads\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_MAX_SIZE = 70000\n",
    "TEXT.build_vocab(data_train, max_size=VOCAB_MAX_SIZE, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train, iter_valid, iter_test = \\\n",
    "    torchtext.data.BucketIterator.splits(datasets=(data_train, data_valid, data_test),\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         device=DEVICE,\n",
    "                                         sort_within_batch=True,\n",
    "                                         sort_key=lambda example: len(example.input),\n",
    "                                         sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, n_vocab, embedding_dim, hidden_dim, output_dim, dropout, bidirectional,\n",
    "                 n_layers, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        num_dir = 2 if bidirectional else 1\n",
    "        self.embedding = torch.nn.Embedding(n_vocab, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
    "                                 hidden_dim,\n",
    "                                 bidirectional=bidirectional,\n",
    "                                 num_layers=n_layers,\n",
    "                                 dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_dim * num_dir, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_lengths):\n",
    "        input, lengths = input_lengths\n",
    "        torch.nn.utils.rnn.pack_padded_sequence(input, lengths)\n",
    "        embedded = self.embedding(input)  # ((sent_len, batch), emb_dim)\n",
    "        packed_output, (hidden, cell) = self.rnn(embedded)  # hidden: (num_layers * num_directions,\n",
    "                                                            #          batch, hidden_size * num_directions)\n",
    "        hidden = (torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1)\n",
    "                  if self.bidirectional else hidden).squeeze(0)  # (batch, hidden_size * num_directions)\n",
    "        return self.sigmoid(self.fc(self.dropout(hidden)))  # (batch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "N_VOCAB = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS = {\n",
    "    \"n_vocab\": N_VOCAB,\n",
    "    \"embedding_dim\": 300,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"output_dim\": 1,\n",
    "    \"dropout\": 0.5,\n",
    "    \"bidirectional\": True,\n",
    "    \"n_layers\": 1,\n",
    "    \"pad_idx\": PAD_IDX,\n",
    "}\n",
    "\n",
    "def default_model(**kwargs):\n",
    "    _d = {}\n",
    "    _d.update(DEFAULT_PARAMS)\n",
    "    _d.update(kwargs)\n",
    "    return RNN(**_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_init(model, criterion, device=DEVICE, learn_embedding_param=True):\n",
    "    model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(model.embedding_dim)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(model.embedding_dim)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name == \"embedding.weight\":\n",
    "            param.requires_grad = learn_embedding_param\n",
    "    \n",
    "    print(\"The model has {:,} trainable parameters\"\n",
    "         .format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_pred(output):\n",
    "    return (output * 10).round() / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/loading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_embedding(path, vocab, embedding):\n",
    "    with open(path, \"w\") as f:\n",
    "        for word, vector in tqdm.tqdm(zip(vocab.itos, embedding)):\n",
    "            \n",
    "            # skip words with unicode symbols\n",
    "            if len(word) != len(word.encode()):\n",
    "                continue\n",
    "            \n",
    "            # 'words' like \" \" or \"\\n\" fail to be loaded\n",
    "            if word.strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            f.write(f\"{word} {' '.join(str(e) for e in vector.tolist())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context(model, field, path_model, path_vocab, cache_embeddings=\"cache_embeddings\",\n",
    "                 batch_size=BATCH_SIZE, device=DEVICE):\n",
    "    \n",
    "    _vectors = torchtext.vocab.Vectors(path_vocab, cache_embeddings)  # voir unk_init\n",
    "    field.build_vocab(data_train, max_size=VOCAB_MAX_SIZE, vectors=_vectors)\n",
    "    \n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    \n",
    "    model.embedding.weight.data.copy_(field.vocab.vectors)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    iter_train, iter_valid, iter_test = \\\n",
    "        torchtext.data.BucketIterator.splits(datasets=(data_train, data_valid, data_test),\n",
    "                                             batch_size=batch_size,\n",
    "                                             device=device,\n",
    "                                             sort_within_batch=True,\n",
    "                                             sort_key=lambda example: len(example.input),\n",
    "                                             sort=False)\n",
    "    \n",
    "    return model, (iter_train, iter_valid, iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_itos(itos, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(itos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_for_pred(folder_path, model=None, itos=None):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    if model is not None:\n",
    "        save_model(model, os.path.join(folder_path, \"model.pt\"))\n",
    "    if itos is not None:\n",
    "        save_itos(itos, os.path.join(folder_path, \"itos.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_for_pred(folder_path, model, device=DEVICE):\n",
    "    model.load_state_dict(torch.load(os.path.join(folder_path, \"model.pt\")))\n",
    "    model = model.to(DEVICE)\n",
    "    with open(os.path.join(folder_path, \"itos.pickle\"), \"rb\") as f:\n",
    "        itos = pickle.load(f)\n",
    "    stoi = collections.defaultdict(lambda: 0)\n",
    "    stoi.update(map(lambda t: t[::-1], enumerate(itos)))\n",
    "    return model, itos, stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    correct = (preds == y).float()  # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.input).squeeze(1)\n",
    "        loss = criterion(output, batch.target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = accuracy(output_to_pred(output), batch.target * 5)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "      \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            output = model(batch.input).squeeze(1)\n",
    "            loss = criterion(output, batch.target)\n",
    "            acc = accuracy(output_to_pred(output), batch.target * 5)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will break easily...\n",
    "class MinMaxList(list):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._min = float(\"inf\")\n",
    "        self._max = -float(\"inf\")\n",
    "    \n",
    "    @property\n",
    "    def min(self):\n",
    "        return self._min\n",
    "    \n",
    "    @property\n",
    "    def max(self):\n",
    "        return self._max\n",
    "    \n",
    "    def append(self, e):\n",
    "        super().append(e)\n",
    "        if e > self._max:\n",
    "            self._max = e\n",
    "        if e < self._min:\n",
    "            self._min = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainInfo:\n",
    "    def __init__(self, valid={}, train={}):\n",
    "        self.valid = collections.defaultdict(MinMaxList)\n",
    "        self.valid.update(valid)\n",
    "        self.train = collections.defaultdict(MinMaxList)\n",
    "        self.train.update(train)\n",
    "    \n",
    "    def save(self, path):\n",
    "        packed = {\n",
    "            \"valid\": dict(self.valid),\n",
    "            \"train\": dict(self.train),\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(packed, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            packed = pickle.load(f)\n",
    "            return cls(valid=packed[\"valid\"],\n",
    "                       train=packed[\"train\"])\n",
    "    \n",
    "    @staticmethod\n",
    "    def _dict_to_repr(d):\n",
    "        return dict(map(lambda k_v: (k_v[0], f\"{len(k_v[1])} elements\"), d.items()))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return pprint.pformat({\"valid\": self._dict_to_repr(self.valid),\n",
    "                     \"train\": self._dict_to_repr(self.train),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(model, name, iter_train, iter_valid, optimizer, criterion, fun_train,\n",
    "                fun_eval, n_epochs=100, training_info=None, fun_stop=lambda: False, fun_save=None):\n",
    "    \n",
    "    if fun_save is None:\n",
    "        print(\"Not saving best model.\")\n",
    "    \n",
    "    clock = simpleclock.Clock.started()\n",
    "    torch.cuda.empty_cache()\n",
    "    train_info = training_info if training_info is not None else TrainInfo()\n",
    "\n",
    "    print(f\"Starting training: {n_epochs} epochs.\")\n",
    "    for epoch in range(n_epochs):\n",
    "        if fun_stop():\n",
    "            print(\"Interrupted. (fun_stop)\")\n",
    "            break\n",
    "\n",
    "        clock.elapsed_since_start.call()  # meh\n",
    "\n",
    "        train_loss, train_acc = fun_train(model, iter_train, optimizer, criterion)\n",
    "        valid_loss, valid_acc = fun_eval(model, iter_valid, criterion)\n",
    "        \n",
    "#         is_best = valid_loss < train_info.valid[\"loss\"].min\n",
    "        is_best = valid_acc > train_info.valid[\"acc\"].max\n",
    "        print(\"Epoch: {e:.<{max_len}}. T, V acc: {train:.1f}%, {valid:.1f}%. Took {t:.2f}s.\"\n",
    "             .format(e=epoch + 1,\n",
    "                     max_len=len(str(n_epochs)),\n",
    "                     train=100 * train_acc,\n",
    "                     valid=100 * valid_acc,\n",
    "                     t=clock.elapsed_since_last_call())\n",
    "             + (\" (+)\" if is_best else \"\")\n",
    "             )\n",
    "        \n",
    "        train_info.train[\"loss\"].append(train_loss)\n",
    "        train_info.valid[\"loss\"].append(valid_loss)\n",
    "        train_info.train[\"acc\"].append(train_acc)\n",
    "        train_info.valid[\"acc\"].append(valid_acc)\n",
    "\n",
    "        if is_best:\n",
    "            if fun_save is not None:\n",
    "                fun_save(model)\n",
    "\n",
    "    clock.elapsed_since_start.print(f\"Trained {name}, {n_epochs} epochs, for\")\n",
    "    return train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSet:\n",
    "    def __init__(self, model, name, iter_train, iter_valid,\n",
    "                 fun_optimizer, fun_criterion, fun_train, fun_eval,\n",
    "                 device=DEVICE, n_epochs=100, training_info=None, fun_stop=lambda: False):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.iter_train = iter_train\n",
    "        self.iter_valid = iter_valid\n",
    "        self.fun_optimizer = fun_optimizer\n",
    "        self.fun_criterion = fun_criterion\n",
    "        self.fun_train = fun_train\n",
    "        self.fun_eval = fun_eval\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "        self.training_info = training_info\n",
    "        self.fun_stop = fun_stop\n",
    "        \n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "        \n",
    "    def init(self, learn_embedding_param=True):\n",
    "        self.model, self.criterion = pseudo_init(self.model, self.fun_criterion(), self.device,\n",
    "                                                 learn_embedding_param=learn_embedding_param)\n",
    "        self.optimizer = self.fun_optimizer(self.model.parameters())\n",
    "    \n",
    "    def do_training(self, fun_save=None):\n",
    "        if self.optimizer is None or self.criterion is None:\n",
    "            raise Exception(\"It looks like an init is needed: optimizer or criterion is None\")\n",
    "        return do_training(model=self.model,\n",
    "                           name=self.name,\n",
    "                           iter_train=self.iter_train,\n",
    "                           iter_valid=self.iter_valid,\n",
    "                           optimizer=self.optimizer,\n",
    "                           criterion=self.criterion,\n",
    "                           fun_train=self.fun_train,\n",
    "                           fun_eval=self.fun_eval,\n",
    "                           n_epochs=self.n_epochs,\n",
    "                           training_info=self.training_info,\n",
    "                           fun_stop=self.fun_stop,\n",
    "                           fun_save=fun_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = []\n",
    "N_EPOCHS = 3\n",
    "\n",
    "for hidden_dim, n_layers in itertools.product([16], [3]):\n",
    "    train_sets.append(TrainSet(\n",
    "        model=default_model(hidden_dim=hidden_dim, n_layers=n_layers),\n",
    "        name=f\"rnn_hidden-{hidden_dim}_nlayers-{n_layers}_nepochs-{N_EPOCHS}\",\n",
    "        iter_train=iter_train,\n",
    "        iter_valid=iter_valid,\n",
    "        fun_optimizer=torch.optim.Adam,\n",
    "        fun_criterion=torch.nn.MSELoss,\n",
    "        fun_train=train,\n",
    "        fun_eval=evaluate,\n",
    "        n_epochs=N_EPOCHS,\n",
    "        fun_stop=lambda: os.path.exists(\".stop\"),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_set in train_sets:\n",
    "    folder_path = os.path.join(os.path.abspath(\"\"), f\"dump_{train_set.name}\")\n",
    "    \n",
    "    train_set.init()\n",
    "    training_info = train_set.do_training()\n",
    "    \n",
    "    # save itos, training_info. model is saved during training\n",
    "    save_for_pred(folder_path, itos=TEXT.vocab.itos)\n",
    "    training_info.save(os.path.join(folder_path, f\"training_info.pickle\"))\n",
    "    \n",
    "    # plot loss data ## todo: handle different data lengths?\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(20, 16))\n",
    "    _n = len(training_info.train[\"loss\"])\n",
    "    ax_train = axs[0].plot(list(range(_n)), training_info.train[\"loss\"], label=\"train\")\n",
    "    ax_valid = axs[0].plot(list(range(_n)), training_info.valid[\"loss\"], label=\"valid\")\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    ax_train = axs[1].plot(list(range(_n)), training_info.train[\"acc\"], label=\"train\")\n",
    "    ax_valid = axs[1].plot(list(range(_n)), training_info.valid[\"acc\"], label=\"valid\")\n",
    "    handles, labels = axs[1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center')\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tokens(tokens, model, stoi, device=DEVICE):\n",
    "    model.eval()\n",
    "    idxs = [stoi[t] for t in tokens]\n",
    "    inp = torch.LongTensor(idxs).reshape(-1, 1).to(device)\n",
    "    output = output_to_pred(model((inp, torch.LongTensor([len(tokens)]))))\n",
    "    return output.item()\n",
    "\n",
    "\n",
    "def predict(sentence, model, stoi=TEXT.vocab.stoi, tokenizer=nlp.tokenizer):\n",
    "    return predict_tokens(list(map(str, tokenizer(sentence))), model, stoi)\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model, stoi=TEXT.vocab.stoi, tokenizer=nlp.tokenizer, device=DEVICE):\n",
    "        self.model = model\n",
    "        self.stoi = stoi\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        return predict(sentence, self.model, self.stoi, self.tokenizer)\n",
    "    \n",
    "    def predict_tokens(self, tokens):\n",
    "        return predict_tokens(tokens, self.model, self.stoi, self.device)\n",
    "    \n",
    "    def __call__(self, sentence):\n",
    "        return self.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, itos, stoi = load_for_pred(\"dump_rnn_hidden-128_nlayers-4_nepochs-1000\",\n",
    "                                  default_model(n_vocab=57051,\n",
    "                                                hidden_dim=128,\n",
    "                                                n_layers=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_set.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model, stoi=stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor(\"Un mauvais film.\"), \\\n",
    "predictor(\"Un bon film.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in random.sample(list(data_train), 1000):\n",
    "    tokens, note = example.input, float(example.target)\n",
    "    if note * 5 != predictor.predict_tokens(tokens):\n",
    "        print(\" \".join(tokens))\n",
    "        print(f\"true) {note * 5} - {predictor.predict_tokens(tokens)} (pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.embedding.weight.data.clone().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_word(word):\n",
    "    return 4 < len(word) < 16\n",
    "\n",
    "N_WORDS = 400\n",
    "\n",
    "word_indexes = [i for i, word in enumerate(itos) if keep_word(word)]\n",
    "word_indexes = random.sample(word_indexes, min(N_WORDS, len(word_indexes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X[word_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(Y)\n",
    "Z = pca.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.scatter(Z[:, 0], Z[:, 1], color=\"white\")\n",
    "for i, row in zip(word_indexes, Z):\n",
    "    ax.text(*row, itos[i], horizontalalignment=\"center\", alpha=.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looser accuracies for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the model really doing bad if it predicts a 4.5 instead of a 5 ? There are at least two ways to allow for forgivable divergence with the test data :\n",
    "* decrease notation's granularity, e.g. tranform the marks into good/bad, or good/bad/neutral.\n",
    "* consider a prediction correct if it belongs to a 'small' interval containing the true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good/Neutral/Bad prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_to_3_way(pred_tensor, bad_treshold=.375, good_treshold=.625):\n",
    "    \"\"\"np array with values: 0: bad, 1: neutral, 2: good\"\"\"\n",
    "    return np.digitize(pred_tensor.cpu().detach().numpy(), [bad_treshold, good_treshold])\n",
    "    \n",
    "def eval_accuracy_3w(model, iterator):\n",
    "    n_examples = 0\n",
    "    n_success = 0\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.input).squeeze(1)\n",
    "        n_examples += len(batch)\n",
    "        n_success += sum(normalized_to_3_way(predictions) == normalized_to_3_way(batch.target))\n",
    "    return n_success / n_examples\n",
    "\n",
    "def classif_report_3w(model, iterator):\n",
    "    def preds_and_trues_to_array(predictions, true_targets):\n",
    "        return np.concatenate([normalized_to_3_way(true_targets).reshape(-1, 1),\n",
    "                               normalized_to_3_way(predictions).reshape(-1, 1)], axis=1)\n",
    "    \n",
    "    array = None\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.input).squeeze(1)\n",
    "        \n",
    "        if array is None:\n",
    "            array = preds_and_trues_to_array(predictions, batch.target)\n",
    "        else:\n",
    "            array = np.concatenate([array,\n",
    "                                    preds_and_trues_to_array(predictions, batch.target)], axis=0)\n",
    "    print(sklearn.metrics.classification_report(array[:, 0],\n",
    "                                                array[:, 1],\n",
    "                                                labels=[0, 1, 2],\n",
    "                                                target_names=[\"bad\", \"neutral\", \"good\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter_test\n",
    "\n",
    "classif_report_3w(model, iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy_fuzzy(model, iterator, fuzziness=.1):\n",
    "    n_examples = 0.\n",
    "    n_success = 0.\n",
    "    for batch in iterator:\n",
    "        predictions = model(batch.input).squeeze(1)\n",
    "        n_examples += len(batch)\n",
    "        n_success += sum(torch.abs(predictions - batch.target) <= fuzziness).item()\n",
    "    return n_success / n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy_fuzzy(model, iter_test, fuzziness=.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, .5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [eval_accuracy_fuzzy(model, iter_test, fuzziness=x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "ax.plot(xs, accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unks = sents = 0\n",
    "for batch in iter_train:\n",
    "    sentences = batch.input[0]\n",
    "    unks += (sentences == 0).sum().item()\n",
    "    sents += batch.batch_size\n",
    "print(f\"avg number of <unk> per sentence: {unks / sents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(itos[10000:10010])\n",
    "print(TEXT.vocab.itos[10000:10010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = set(itos).symmetric_difference(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
